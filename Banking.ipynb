{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Client behavior prediction with AI model in Banking**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this laboratory is to develop different types of classifiers of artificial intelligence and their ensembles for the classification of customers in banking.\n",
    "\n",
    "After completing this laboratory work you will be able to:\n",
    "\n",
    "1. compare different types of classifiers\n",
    "2. create an ensemble of models\n",
    "3. create a ensemble of classifier based on neural networks\n",
    "4. to make classification of clients on the basis of the developed models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Materials and Methods\n",
    "* General Part\n",
    "  * Import Libraries\n",
    "  * DataSet preparation\n",
    "  * Classical classification models\n",
    "  * Ensemble of classical classification models\n",
    "  * Analysis of errors\n",
    "  * Classification with Keras\n",
    "  * Own Keras ensemble classificator\n",
    "* Tasks\n",
    "* Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materials and Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we are going to use for this is a subset of an open source Bank Marketing Data Set from the UCI ML repository: https://archive.ics.uci.edu/.\n",
    "\n",
    "> This dataset is public available for research. The details are described in [Moro et al., 2014].\n",
    "Please include this citation if you plan to use this database:\n",
    "[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n",
    "\n",
    "During the work, the task of a preliminary analysis of a positive response (term deposit) to direct calls from the bank is solved. In essence, the task is the matter of bank scoring, i.e. according to the characteristics of clients (potential clients), their behavior is predicted (loan default, a wish to open a deposit, etc.).\n",
    "\n",
    "In this lesson, we will try to give answers to a set of questions that may be relevant when analyzing banking data:\n",
    "\n",
    "1. What types of classificators are useful in banking?\n",
    "2. How to create ensemble of models?\n",
    "4. How to create own classificator base on neural network?\n",
    "5. How to estimate accurasy of model?\n",
    "\n",
    "In addition, we will make the conclusions for the obtained results of our classification analysis to plan marketing banking campaigns more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Keras](https://keras.io) Keras is the most used deep learning framework among top-5 winning teams on Kaggle. Because Keras makes it easier to run new experiments, it empowers you to try more ideas than your competition, faster. And this is how you win.Built on top of TensorFlow 2, Keras is an industry-strength framework that can scale to large clusters of GPUs or an entire TPU pod. It's not only possible; it's easy.Take advantage of the full deployment capabilities of the TensorFlow platform. You can export Keras models to JavaScript to run directly in the browser, to TF Lite to run on iOS, Android, and embedded devices. It's also easy to serve Keras models as via a web API. Keras is a central part of the tightly-connected TensorFlow 2 ecosystem, covering every step of the machine learning workflow, from data management to hyperparameter training to deployment solutions.Keras is used by CERN, NASA, NIH, and many more scientific organizations around the world (and yes, Keras is used at the LHC). Keras has the low-level flexibility to implement arbitrary research ideas while offering optional high-level convenience features to speed up experimentation cycles. Because of its ease-of-use and focus on user experience, Keras is the deep learning solution of choice for many university courses. It is widely recommended as one of the best ways to learn deep learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data using a URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-10 05:38:40--  https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified\n",
      "Saving to: ‘bank-additional.zip’\n",
      "\n",
      "bank-additional.zip     [  <=>               ] 434.15K  1.23MB/s    in 0.3s    \n",
      "\n",
      "2023-11-10 05:38:41 (1.23 MB/s) - ‘bank-additional.zip’ saved [444572]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative URL for the dataset downloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-10 05:38:46--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VDA_Banking_L2/bank-additional.zip\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 444572 (434K) [application/zip]\n",
      "Saving to: ‘bank-additional.zip.1’\n",
      "\n",
      "bank-additional.zip 100%[===================>] 434.15K  --.-KB/s    in 0.008s  \n",
      "\n",
      "2023-11-10 05:38:47 (51.1 MB/s) - ‘bank-additional.zip.1’ saved [444572/444572]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/VDA_Banking_L2/bank-additional.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzipping to a folder. It is a good idea to apply the `-o` and `-q`  when unzipping to quiet the process and overwrite any existing folders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -o -q bank-additional.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries necessary to use in this lab. We can add some aliases to make the libraries easier to use in our code and set a default figure size for further plots. Ignore the warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  __    __    __    __\n",
      "                 /  \\  /  \\  /  \\  /  \\\n",
      "                /    \\/    \\/    \\/    \\\n",
      "███████████████/  /██/  /██/  /██/  /████████████████████████\n",
      "              /  / \\   / \\   / \\   / \\  \\____\n",
      "             /  /   \\_/   \\_/   \\_/   \\    o \\__,\n",
      "            / _/                       \\_____/  `\n",
      "            |/\n",
      "        ███╗   ███╗ █████╗ ███╗   ███╗██████╗  █████╗\n",
      "        ████╗ ████║██╔══██╗████╗ ████║██╔══██╗██╔══██╗\n",
      "        ██╔████╔██║███████║██╔████╔██║██████╔╝███████║\n",
      "        ██║╚██╔╝██║██╔══██║██║╚██╔╝██║██╔══██╗██╔══██║\n",
      "        ██║ ╚═╝ ██║██║  ██║██║ ╚═╝ ██║██████╔╝██║  ██║\n",
      "        ╚═╝     ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝╚═════╝ ╚═╝  ╚═╝\n",
      "\n",
      "        mamba (1.4.2) supported by @QuantStack\n",
      "\n",
      "        GitHub:  https://github.com/mamba-org/mamba\n",
      "        Twitter: https://twitter.com/QuantStack\n",
      "\n",
      "█████████████████████████████████████████████████████████████\n",
      "\n",
      "\n",
      "Looking for: ['scikit-learn']\n",
      "\n",
      "\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/main/noarch   \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/r/linux-64    \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\n",
      "pkgs/r/noarch      \u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.2s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m  28.7kB /  ??.?MB @ 186.2kB/s  0.2s\n",
      "pkgs/main/noarch   \u001b[90m━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m  28.7kB /  ??.?MB @ 186.4kB/s  0.2s\n",
      "pkgs/r/linux-64    \u001b[90m━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m  57.4kB /  ??.?MB @ 372.1kB/s  0.2s\n",
      "pkgs/r/noarch      \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m  28.7kB /  ??.?MB @ 185.6kB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.3s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━\u001b[0m 376.8kB /  ??.?MB @   1.5MB/s  0.3s\n",
      "pkgs/main/noarch   \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m 553.0kB /  ??.?MB @   2.2MB/s  0.3s\n",
      "pkgs/r/linux-64    \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m 565.3kB /  ??.?MB @   2.2MB/s  0.3s\n",
      "pkgs/r/noarch      \u001b[90m━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m 458.8kB /  ??.?MB @   1.8MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch                                   853.2kB @   2.8MB/s  0.3s\n",
      "[+] 0.4s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m 938.0kB /  ??.?MB @   2.6MB/s  0.4s\n",
      "pkgs/r/linux-64    \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   1.1MB /  ??.?MB @   3.0MB/s  0.4s\n",
      "pkgs/r/noarch      \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m 921.6kB /  ??.?MB @   2.6MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m   1.4MB /  ??.?MB @   3.1MB/s  0.5s\n",
      "pkgs/r/linux-64    \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━\u001b[0m   1.6MB /  ??.?MB @   3.5MB/s  0.5s\n",
      "pkgs/r/noarch      \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   1.4MB /  ??.?MB @   3.0MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/linux-64                                      1.9MB @   3.7MB/s  0.5s\n",
      "[+] 0.6s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m   1.9MB /  ??.?MB @   3.4MB/s  0.6s\n",
      "pkgs/r/noarch      \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━\u001b[0m   1.9MB /  ??.?MB @   3.3MB/s  0.6s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch                                        2.3MB @   3.5MB/s  0.7s\n",
      "[+] 0.7s\n",
      "pkgs/main/linux-64 \u001b[90m╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   2.7MB /  ??.?MB @   3.8MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.8s\n",
      "pkgs/main/linux-64 \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m   2.9MB /  ??.?MB @   3.9MB/s  0.8s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.9s\n",
      "pkgs/main/linux-64 \u001b[90m━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m   3.4MB /  ??.?MB @   4.0MB/s  0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   3.9MB /  ??.?MB @   4.1MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━━\u001b[0m   4.5MB /  ??.?MB @   4.2MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.2s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   5.1MB /  ??.?MB @   4.4MB/s  1.2s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.3s\n",
      "pkgs/main/linux-64 \u001b[90m━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m   5.6MB /  ??.?MB @   4.4MB/s  1.3s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.4s\n",
      "pkgs/main/linux-64 \u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m   6.2MB /  ??.?MB @   4.5MB/s  1.4s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.5s\n",
      "pkgs/main/linux-64 ━━━━━━━━━━━━━━━━━━━━━━━━   6.3MB @   4.5MB/s Finalizing  1.5s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.6s\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64                                 @   4.5MB/s  1.5s\n",
      "\u001b[?25h\n",
      "Pinned packages:\n",
      "  - python 3.7.*\n",
      "\n",
      "\n",
      "Transaction\n",
      "\n",
      "  Prefix: /home/jupyterlab/conda/envs/python\n",
      "\n",
      "  Updating specs:\n",
      "\n",
      "   - scikit-learn\n",
      "   - ca-certificates\n",
      "   - certifi\n",
      "   - openssl\n",
      "\n",
      "\n",
      "  Package               Version  Build           Channel                 Size\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "  Install:\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  \u001b[32m+ joblib         \u001b[0m       1.1.1  py37h06a4308_0  pkgs/main/linux-64     388kB\n",
      "  \u001b[32m+ scikit-learn   \u001b[0m       1.0.2  py37h51133e4_1  pkgs/main/linux-64       6MB\n",
      "  \u001b[32m+ threadpoolctl  \u001b[0m       2.2.0  pyh0d69192_0    pkgs/main/noarch        17kB\n",
      "\n",
      "  Upgrade:\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  \u001b[31m- ca-certificates\u001b[0m    2023.5.7  hbcca054_0      conda-forge                 \n",
      "  \u001b[32m+ ca-certificates\u001b[0m  2023.08.22  h06a4308_0      pkgs/main/linux-64     125kB\n",
      "  \u001b[31m- openssl        \u001b[0m      1.1.1t  h0b41bf4_0      conda-forge                 \n",
      "  \u001b[32m+ openssl        \u001b[0m      1.1.1w  h7f8727e_0      pkgs/main/linux-64       4MB\n",
      "\n",
      "  Summary:\n",
      "\n",
      "  Install: 3 packages\n",
      "  Upgrade: 2 packages\n",
      "\n",
      "  Total download: 10MB\n",
      "\n",
      "───────────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "Confirm changes: [Y/n] "
     ]
    }
   ],
   "source": [
    "!mamba install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "# Data transformation\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Features Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif\n",
    "# warnings deactivate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further specify the value of the `precision` parameter equal to 2 to display two decimal signs (instead of 6 as default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"precision\", 2)\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same DataSet like in previous labs. Therefore next some steps will be group to one block.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank-additional/bank-additional-full.csv', sep = ';')\n",
    "# Transform to categorical data\n",
    "col_cat = list(df.select_dtypes(include=['object']).columns)\n",
    "df.loc[:, col_cat] = df[col_cat].astype('category')\n",
    "# Create DataSets\n",
    "X = df.iloc[:,:-1]  #input columns\n",
    "y = df.iloc[:,-1]    #target column \n",
    "# Encoding\n",
    "col_cat.pop()\n",
    "oe = OrdinalEncoder()\n",
    "oe.fit(X[col_cat])\n",
    "X_cat_enc = oe.transform(X[col_cat])\n",
    "X_cat_enc = pd.DataFrame(X_cat_enc)\n",
    "X_cat_enc.columns = col_cat\n",
    "# Normalization\n",
    "col_num = list(df.select_dtypes(include =['int64', 'float64']).columns)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_num_enc = scaler.fit_transform(X[col_num])\n",
    "X_num_enc = pd.DataFrame(X_num_enc)\n",
    "X_num_enc.columns = col_num\n",
    "x_enc = pd.concat([X_cat_enc, X_num_enc], axis=1)\n",
    "# Encoding target\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "y_enc = le.transform(y)\n",
    "y_enc = pd.Series(y_enc)\n",
    "y_enc.columns = y.name\n",
    "# Remove correlated fields\n",
    "col = list(x_enc.columns)\n",
    "col.remove('emp.var.rate')\n",
    "col.remove('nr.employed')\n",
    "x_enc = x_enc[col]\n",
    "# Feature selection\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(x_enc,y_enc)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(x_enc.columns)\n",
    "featureScores = pd.concat([dfcolumns, dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "x_enc = x_enc[list(featureScores.nlargest(10,'Score')['Specs'])]\n",
    "print(x_enc)\n",
    "print(y_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we obtained a ready-made DataSet that can be used for further classification of bank customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical classification models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Classifiers\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use 2 approaches to determine the accuracy of the model. the first is to divide the data set into training and test. This was shown in previous laboratory work. The second is evaluating a score by cross-validation.\n",
    "\n",
    "[Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\n",
    "\n",
    "In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **[cross_val_score()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)** for estimate accuracy of any classificator from **sklearn** framework.\n",
    "\n",
    "Let's test it on LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "scores = cross_val_score(clf, x_enc, y_enc, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Logistic Regression'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare results of different classificators, let's save results in lists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name = ['Logistic Regression']\n",
    "clf_acc = [scores.mean()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, accuracy is not bad.\n",
    "\n",
    "Let's try to use another classificators and compare thair accuracy.\n",
    "\n",
    "We will test:\n",
    "* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)\n",
    "* [Quadratic Discriminant Analysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html?highlight=quadraticdiscriminantanalysis#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis)\n",
    "* [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html?highlight=gaussiannb#sklearn.naive_bayes.GaussianNB)\n",
    "* [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforestclassifier#sklearn.ensemble.RandomForestClassifier)\n",
    "* [Ada Boost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html?highlight=adaboostclassifier#sklearn.ensemble.AdaBoostClassifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = QuadraticDiscriminantAnalysis()\n",
    "clf3 = GaussianNB()\n",
    "clf4 = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
    "clf5 = AdaBoostClassifier()\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, clf4, clf5], ['Logistic Regression', 'Quadratic Discriminant Analysis', 'naive Bayes', 'Random Forest', 'Ada Boost']):\n",
    "     scores = cross_val_score(clf, x_enc, y_enc, scoring='accuracy', cv=5)\n",
    "     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of classical classification models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, different classifiers may err in different situations. Therefore, to compensate for each other's mistakes, it is necessary to use model ensembles by Voting Classifier.\n",
    "\n",
    "A **[Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)** is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\n",
    "It simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n",
    "\n",
    "Voting Classifier supports two types of votings.\n",
    "\n",
    "**Hard Voting**: In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n",
    "\n",
    "\n",
    "**Soft Voting**: In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use Hard Voiting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), ('rfc', clf4), ('abc', clf5)],\n",
    "     voting='hard')\n",
    "scores = cross_val_score(eclf, x_enc, y_enc, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Hard Voiting Ensemble'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name.append('Hard Voiting Ensemble')\n",
    "clf_acc.append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, accuracy of ensembe is smaler than for Logistic regression but bigger than for Ada Bust.\n",
    "To increase accuracy, it is necessary to use Soft Voiting and take into account the accuracy of classification models, indicating their importance (weights).\n",
    "\n",
    "As weights we can use accuracy of each model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(\n",
    "     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), ('rfc', clf4), ('abc', clf5)],\n",
    "     voting='soft', weights=[0.87, 0.83, 0.80, 0.74, 0.61 ])\n",
    "\n",
    "scores = cross_val_score(eclf, x_enc, y_enc, scoring='accuracy', cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), 'Soft Voiting Ensemble'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "eclf = VotingClassifier(\n",
    "     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3), ('rfc', clf4), ('abc', clf5)],\n",
    "     voting='soft', weights=[0.87, 0.83, 0.80, 0.74, 0.61 ])\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name.append('Soft Voiting Ensemble')\n",
    "clf_acc.append(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, accuracy of this ensemble is better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **[convenient matrix](https://scikit-learn.org/1.0/modules/generated/sklearn.metrics.plot_confusion_matrix.html)** can use to analyze ensemble of models errors.\n",
    "\n",
    "Cross-validation does not allow you to make such an analysis, because it creates not one but several models depending on the number of partitions in the DataSet. Therefore, it is necessary to divide the data into test and train, fit the model and check on the test DataSet\n",
    "To do this we can use train_test_split. Let's separate DataSets in 0.33 proportion train/test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_enc, y_enc, test_size=0.33, random_state=1)\n",
    "eclf.fit(X_train, y_train)\n",
    "yhat = eclf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_train = eclf.score(X_train, y_train)\n",
    "scores_test = eclf.score(X_test, y_test)\n",
    "print('Training DataSet accuracy: {: .1%}'.format(scores_train), 'Test DataSet accuracy: {: .1%}'.format(scores_test))\n",
    "plot_confusion_matrix(eclf, X_test, y_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see accuracy test and train DataSet is the same. It indicates, that models is exact. And for increasing accuracy you should add new rows in DataSet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic architecture of the deep learning neural network, which we will be following, consists of three main components.\n",
    "\n",
    "**Input Layer**: This is where the training observations are fed. The number of predictor variables is also specified here through the neurons.\n",
    "\n",
    "**Hidden Layers**: \n",
    "These are the intermediate layers between the input and output layers. The deep neural network learns about the relationships involved in data in this component.In our cases hidden layer will have 20 neurons.\n",
    "\n",
    "**Output Layer**: This is the layer where the final output is extracted from what’s happening in the previous two layers. In case of classification problems, the output layer will have two neuron because 2 classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by setting up the model like a function. The first line of code calls for the Sequential constructor. We are using the Sequential model because our network consists of a linear stack of layers.\n",
    "The second line of code represents the hidden layer where we must sign number of hidden neurons which specifies the activation function and the number of input dimensions, which in our case is 10 predictors. \n",
    "The next line of code creates the output layer with two neurons because there are two output classes, 0 and 1. We use 'softmax' as the activation function for the output layer, so that the sum of the predicted values from all the neurons in the output layer adds up to one.\n",
    "In the above lines of codes, we have defined our deep learning model architecture. But before we can start training the model, we will configure the learning process. This is done in the last line of code using the model.compile() function.\n",
    "In defining our compiler, we will use 'categorical cross-entropy' as our loss measure, 'adam' as the optimizer algorithm, and 'accuracy' as the evaluation metric. The main advantage of the \"adam\" optimizer is that we don't need to specify the learning rate, as is the case with gradient descent. Using “adam” will, thereby, save us the task of optimizing the learning rate for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=10, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build the model which is done in the code below. We also provide the argument, epochs, which represents the number of training iterations. We have taken 10 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=5, verbose=1)\n",
    "history = estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides the capability to register callbacks when training a deep learning model.\n",
    "\n",
    "One of the default callbacks that is registered when training all deep learning models is the History callback. It records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) as well as the loss and accuracy for the validation dataset, if one is set.\n",
    "\n",
    "The history object is returned from calls to the fit() function used to train the model. Metrics are stored in a dictionary in the history member of the object returned.\n",
    "\n",
    "For example, you can list the metrics collected in a history object using the following snippet of code after a model is trained:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs_range = range(10)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the plots:  accuracy has reached a maximum and loss funcion on minimum. It means that model is good fitted and can use for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When model is fitted we can predict on the Test Data and Compute Evaluation Metrics.\n",
    "\n",
    "The first line of code predicts on the test data, while the second line evaluates the model, and the third line prints the accuracy on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = estimator.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "yhat = estimator.predict(X_test)\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name.append('KerasClassifier')\n",
    "clf_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see accurasy of this model is better than all previous\n",
    "\n",
    "Also we can create a confusion matrix. Unfortunately, Keras framework does not have plot_confusion_matrix() function. Therefore, we have to create it using Pandas and **[Seaborn.heatmap()](https://seaborn.pydata.org/generated/seaborn.heatmap.html)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Confusion matrix\n",
    "cm = pd.DataFrame(confusion_matrix(yhat, y_test))\n",
    "sns.heatmap(cm, annot=True, )\n",
    "plt.xlabel(\"True labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easy use this model, that will use all 10 fields as input parameter to calculate class of clients.\n",
    "\n",
    "As you can see, own classificator is more exact, but fitting process is too long. Unfortunatelly you cannot use this estimator for VotingClassifier.\n",
    "\n",
    "To use ensembe of similar classificators you can use **[BaggingClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier \n",
    "ensemble = BaggingClassifier(estimator, n_estimators=3, max_samples=1.0, max_features=1.0)\n",
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we created ensemble, that consists from 3 identical Keras Classificators. Fitting process are using random processes therefore accuracy of these 3 Classificators will be little different. But ensamble must show the same or bettter result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = ensemble.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.4f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "yhat = ensemble.predict(X_test)\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name.append('BaggingClassifier')\n",
    "clf_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see result is little better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own Keras ensemble classificator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like use ensamble of different models you can create own ensembe.\n",
    "To do this you must:\n",
    "1. Create list of fitted classificators\n",
    "2. Join their outputts\n",
    "3. Use these outputs like input for stack classificator\n",
    "\n",
    "Let's do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we must create list of Keras classificators. If you would like, youe can use different classificators. But for comparison of accuracy we will use the same keras models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit and save models\n",
    "n_members = 3\n",
    "members = list()\n",
    "for i in range(n_members):\n",
    "    # fit model\n",
    "    model = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=5, verbose=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    members.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calcelete accurasy of these models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate standalone models on test dataset\n",
    "for model in members:\n",
    "    yhat = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    print('Accuracy: %.4f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Than we must join outputs of these models into one DataSet that will use like input for stack classificator. Let's create function, that will use ensemble models and input DataSet like input parameters and return join results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create stacked model input dataset as outputs from the ensemble\n",
    "from numpy import dstack\n",
    "def stacked_dataset(members, inputX):\n",
    "    stackX = None\n",
    "    for model in members:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        # stack predictions into [rows, members, probabilities]\n",
    "        if stackX is None:\n",
    "            stackX = yhat\n",
    "        else:\n",
    "            stackX = dstack((stackX, yhat))\n",
    "    # flatten predictions to [rows, members]\n",
    "    stackX = stackX.reshape((stackX.shape[1], stackX.shape[2]))\n",
    "    return stackX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we must create stack classificator that combine results of ensemble members. Let's use LogisticRegression for it and create function that will fit it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble members\n",
    "def fit_stacked_model(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While funcion are created, we can fit stack ensembe model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(members, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate prediction we must create function, that will use members, stack function and input DataSet like input parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a prediction with the stacked model\n",
    "def stacked_prediction(members, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make prediction of our own ensemble:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = stacked_prediction(members, model, X_test)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.4f' % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this accurasy is better!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf_name.append('LinearKerasEnsemble')\n",
    "clf_acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare results of our ensembles:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Classificators':clf_name, 'Accuracy':clf_acc})\n",
    "ax = df.plot.bar(x='Classificators', y='Accuracy', rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see accurasy of own Classificator Ensemble is the best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we compared different classifiers. Also we learned how to join them in an ensemble.\n",
    "We also learned to create our own classifiers based on neural networks and combine them into an ensemble.\n",
    "We compared the accuracy of different classifiers and their ensembles and showed how they can be used in banking on the examples of customer classification.\n",
    "\n",
    "The accuracy of the decision was about 90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create own stack Classificator base on Keras Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1** Create user function that defined baseline stacked classificator model based on Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stacked_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=n_members, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "def stacked_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=n_members, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2** Create user function that defined and fit a model based on the outputs from the ensemble members based on Keras Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stacked_model_NN(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    model = KerasClassifier(build_fn=stacked_model, epochs=10, batch_size=5, verbose=1)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "def fit_stacked_model_NN(members, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = stacked_dataset(members, inputX)\n",
    "    # fit standalone model\n",
    "    model = KerasClassifier(build_fn=stacked_model, epochs=10, batch_size=5, verbose=1)\n",
    "    model.fit(stackedX, inputy)\n",
    "    return model\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3** Fit stacked model using the ensemble and calculate accurasy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "model = fit_stacked_model_NN(members, X_test, y_test)\n",
    "yhat = stacked_prediction(members, model, X_test)\n",
    "accuracy = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.2f' % (accuracy))\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4** Compare results of all classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##YOUR CODE GOES HERE##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Click <b>here</b> for the solution</summary> \n",
    "<code>    \n",
    "clf_name.append('NNKerasEnsemble')\n",
    "clf_acc.append(accuracy)\n",
    "df = pd.DataFrame({'Classificators':clf_name, 'Accuracy':clf_acc})\n",
    "ax = df.plot.bar(x='Classificators', y='Accuracy', rot=45)\n",
    "df\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yaroslav Vyklyuk, prof., PhD., DrSc](https://author.skills.network/instructors/yaroslav_vyklyuk_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Copyright &copy; 2021 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
